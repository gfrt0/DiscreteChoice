using BenchmarkTools, Distributions, LinearAlgebra, StatsBase, Optim

### Data setup 

N = 5000 # number of simulated individuals
J = 3    # number of inside choices
R = 100  # simulated shocks per individual


Œº = [-1, .5]
Œ£ = [.5 0; 0 .9]

Œ≤·µ¢ = rand(MvNormal(Œº, Œ£), N)

p·µ¢‚±º = rand(Exponential(1), N, J) # characteristics vary at the individual level.
x·µ¢‚±º = [2.2 3.8 1.4] .+ rand(Exponential(2.5), N, J)

s·µ¢‚±º = zeros(Float64, N, J + 1)

# choice probabilities under the model 
v·µ¢‚±º = Œ≤·µ¢[1, :] .* p·µ¢‚±º + Œ≤·µ¢[2, :] .* x·µ¢‚±º
s·µ¢‚±º = [ exp.(v·µ¢‚±º) ones(Float64, N) ]
s·µ¢‚±º = s·µ¢‚±º ./ sum(s·µ¢‚±º, dims = 2)

œÉ‚±º = mean(s·µ¢‚±º, dims = 1)

# observed choices
y·µ¢‚±º = zeros(Float64, N, J + 1)
for i ‚àà 1:N
    y·µ¢‚±º[i, sample(collect(1:(J + 1)), Weights(s·µ¢‚±º[i, :]), 1)] .= 1.0 
end

# objects useful below 
p·µ¢·µ£‚±º = repeat(p·µ¢‚±º, inner = (R, 1))
x·µ¢·µ£‚±º = repeat(x·µ¢‚±º, inner = (R, 1))
Œ≤·µ¢·µ£ = zeros(Float64, 2, N * R)

####################################
### maximum simulated likelihood ###
####################################

# T(a, b) = [exp(max(a, -4.0)) 0.0; 0.0 exp(max(b, -4.0))] * [exp(max(a, -4.0)) 0.0; 0.0 exp(max(b, -4.0))]'
T(a, b) = [exp(a) 0.0; 0.0 exp(b)] * [exp(a) 0.0; 0.0 exp(b)]'
T(Œ∏) = T(Œ∏[3], Œ∏[4])

o·µ¢·µ£ = rand(MvNormal([0.0, 0.0], [1.0 0.0; 0.0 1.0]), N * R)

function s·µ¢‚±º_sim(œë, Œ≤·µ¢·µ£) 
    Œ≤·µ¢·µ£  .= œë[1:2] .+ T(œë) * o·µ¢·µ£ 
    @views s·µ¢·µ£‚±º  = [exp.(Œ≤·µ¢·µ£[1, :] .* p·µ¢·µ£‚±º + Œ≤·µ¢·µ£[2, :] .* x·µ¢·µ£‚±º) ones(Float64, N * R)]
    s·µ¢·µ£‚±º .= s·µ¢·µ£‚±º ./ sum(s·µ¢·µ£‚±º, dims = 2) 
    return dropdims(mean(reshape(s·µ¢·µ£‚±º, R, N, J + 1), dims = 1), dims = 1)
end 

ln‚Ñí_msl(œë) = - sum( log.( sum( y·µ¢‚±º .* s·µ¢‚±º_sim(œë, Œ≤·µ¢·µ£), dims = 2 ) ) )

MSL = optimize(ln‚Ñí_msl, [ -0.25 0.25 -.7 -.7 ], NelderMead(), 
               Optim.Options(g_tol = 1e-12, time_limit = 100, show_trace = true, show_every = 10))
Œ∏MSL = Optim.minimizer( MSL )
[Œ∏MSL[1:2]... diag(T(Œ∏MSL))...]
[Œº... diag(Œ£)...] # diag(Œ£) terms hard to get right  

########################################################
### importance sampling with normal proposal density ###
########################################################

function s·µ¢·µ£‚±º_is(œë, o·µ¢·µ£)
    Œ≤·µ¢·µ£  .= o·µ¢·µ£
    @views s·µ¢·µ£‚±º = [exp.(Œ≤·µ¢·µ£[1, :] .* p·µ¢·µ£‚±º + Œ≤·µ¢·µ£[2, :] .* x·µ¢·µ£‚±º) ones(Float64, N * R)]
    s·µ¢·µ£‚±º .= s·µ¢·µ£‚±º ./ sum(s·µ¢·µ£‚±º, dims = 2) 
    ùíª·µ¢·µ£   = pdf(MvNormal([œë[1], œë[2]], T(œë)), o·µ¢·µ£) # density under proposal density
    return s·µ¢·µ£‚±º, ùíª·µ¢·µ£
end 
 
function s·µ¢‚±º_is(œë, s·µ¢·µ£‚±º, ùíª·µ¢·µ£, o·µ¢·µ£)
    ‚Ñä·µ¢·µ£ = pdf(MvNormal([œë[1], œë[2]], T(œë)), o·µ¢·µ£)    
    ùìà·µ¢·µ£‚±º = s·µ¢·µ£‚±º .* ‚Ñä·µ¢·µ£ ./ ùíª·µ¢·µ£
    return dropdims(mean(reshape(ùìà·µ¢·µ£‚±º, R, N, J + 1), dims = 1), dims = 1)
end 

ln‚Ñí_is(œë, y·µ¢‚±º, s·µ¢·µ£‚±º, ùíª·µ¢·µ£, o·µ¢·µ£) = - sum( log.( sum( y·µ¢‚±º .* s·µ¢‚±º_is(œë, s·µ¢·µ£‚±º, ùíª·µ¢·µ£, o·µ¢·µ£), dims = 2 ) ) )

function importancesampling(proposal_theta)
  
    rŒ∏ = round.([proposal_theta[1:2]... diag(T(proposal_theta))...], digits = 3)
    print("Proposal distribution parameters: $rŒ∏.\n")

    o·µ¢·µ£ = rand(MvNormal([proposal_theta[1], proposal_theta[2]], T(proposal_theta)), N * R) # draws from proposal density
  
    s·µ¢·µ£‚±º, ùíª·µ¢·µ£ = s·µ¢·µ£‚±º_is(proposal_theta, o·µ¢·µ£)
      
    IS = optimize(œë -> ln‚Ñí_is(œë, y·µ¢‚±º, s·µ¢·µ£‚±º, ùíª·µ¢·µ£, o·µ¢·µ£), [ -0.25 0.25 -.7 -.7 ], NelderMead(), 
                  Optim.Options(g_tol = 1e-12, time_limit = 100, show_trace = true, show_every = 10))
    ISt = Optim.minimizer(IS)

    return [ISt[1:2]... diag(T(ISt))...]
end 

IS1 = importancesampling([ 0.0 0.0 0.0 0.0 ])
IS2 = importancesampling([ -0.25 0.25 log(0.5) log(0.5) ])
IS3 = importancesampling([Œº... log.(diag(cholesky(Œ£).factors))...])

[Œº... diag(Œ£)...]
IS1 
IS2   # a good proposal distribution is extremely important for the performance of the estimator.
IS3 

######################################################
### iteratively updating the proposal distribution ###
######################################################

function importancesampling_iterative(proposal_theta; time_limit = 100000.0, nupdates = 1)
  
    time_run = 0.0 
    g_converged = false
    Œ∏ = proposal_theta
    iter = 0

    u·µ¢·µ£ = rand(MvNormal([0.0, 0.0], [1.0 0.0; 0.0 1.0]), N * R) # draws from N(0, I) to be held fixed  
    
    while (time_run < time_limit) & (g_converged == false) & (iter < nupdates)

        rŒ∏ = round.([Œ∏[1:2]... diag(T(Œ∏))...], digits = 3)
        print("Proposal distribution updated to $rŒ∏. Runtime: $time_run. \n")

        o·µ¢·µ£ = Œ∏[1:2] .+ T(Œ∏) * u·µ¢·µ£ # updating draws from proposal density using affine transformation
    
        s·µ¢·µ£‚±º, ùíª·µ¢·µ£ = s·µ¢·µ£‚±º_is(Œ∏, o·µ¢·µ£)
            
        IS = optimize(œë -> ln‚Ñí_is(œë, y·µ¢‚±º, s·µ¢·µ£‚±º, ùíª·µ¢·µ£, o·µ¢·µ£), [ -0.25 0.25 -.7 -.7 ], NelderMead(), 
                      Optim.Options(g_tol = 1e-12, time_limit = time_limit, show_trace = true, show_every = 5))
        Œ∏ = Optim.minimizer( IS )
        
        g_converged = IS.g_converged
        time_run += Optim.time_run(IS)
        iter += 1
    end 

    # after nupdates resets, let the last optimization run to conclusion 
    rŒ∏ = round.([Œ∏[1:2]... diag(T(Œ∏))...], digits = 3)
    print("Proposal distribution finally updated to $rŒ∏. Runtime: $time_run. \n")

    o·µ¢·µ£ = Œ∏[1:2] .+ T(Œ∏) * u·µ¢·µ£

    s·µ¢·µ£‚±º, ùíª·µ¢·µ£ = s·µ¢·µ£‚±º_is(Œ∏, o·µ¢·µ£)
    
    IS = optimize(œë -> ln‚Ñí_is(œë, y·µ¢‚±º, s·µ¢·µ£‚±º, ùíª·µ¢·µ£, o·µ¢·µ£), Œ∏, NelderMead(), 
                  Optim.Options(g_tol = 1e-12, time_limit = 2 * time_limit, show_trace = true, show_every = 5))
    Œ∏ = Optim.minimizer( IS )

    return Œ∏, [Œ∏[1:2]... diag(T(Œ∏))...]
end 

ISI1, ISI1t = importancesampling_iterative([ 0.0 0.0 0.0 0.0 ])
ISI3, ISI3t = importancesampling_iterative([Œº... log.(diag(cholesky(Œ£).factors))...])

ISI1t
ISI3t